import argparse
import json
import shutil
import time
from pathlib import Path
import numpy as np
import cv2
import torch
import tqdm
from lerobot.datasets.lerobot_dataset import LeRobotDataset

import sys
# ç¡®ä¿åŒ…å« openpi æ–‡ä»¶å¤¹çš„ä¸Šä¸€å±‚ç›®å½• (src) åœ¨è·¯å¾„ä¸­
sys.path.append("/home/openpi/src")
from openpi.shared import image_tools

# é…ç½®é¡¹
SCALE_FACTOR = 1.0
image_resolution = (224, 224)
SCAN_INTERVAL = 5  # æ¯éš”5ç§’æ‰«æä¸€æ¬¡æ–°å¢episode
MAX_RETRY = 3      # å•ä¸ªepisodeè½¬æ¢å¤±è´¥é‡è¯•æ¬¡æ•°

# æ–°å¢ï¼šåˆ›å»ºç©ºå›¾åƒï¼ˆç”¨äºå¡«å……ç¼ºå¤±çš„æ‘„åƒå¤´æ•°æ®ï¼‰
def create_empty_image(resolution):
    """åˆ›å»ºé»‘è‰²ç©ºå›¾åƒï¼ˆRGBæ ¼å¼ï¼‰"""
    H, W = resolution
    return np.zeros((H, W, 3), dtype=np.uint8)

def create_dataset(repo_id, root_dir, robot_type="xarm", incremental=False):
    """åˆ›å»º/åŠ è½½æ•°æ®é›†ï¼ˆå¢é‡æ¨¡å¼ä¸åˆ é™¤æ—§æ•°æ®ï¼‰"""
    root_dir = Path(root_dir)
    output_path = root_dir / repo_id
    
    # å¢é‡æ¨¡å¼ï¼šä¿ç•™åŸæœ‰æ•°æ®ï¼Œä»…åŠ è½½ï¼›å…¨é‡æ¨¡å¼ï¼šæ¸…ç©ºé‡å»º
    if not incremental and output_path.exists():
        print(f"[å…¨é‡æ¨¡å¼] æ¸…ç†æ—§æ•°æ®é›†: {output_path}")
        shutil.rmtree(output_path)

    features = {
        # # ç»å¯¹å€¼
        # "observation.state": {
        #     "dtype": "float32",
        #     "shape": (7,), 
        #     "names": ["joint1", "joint2", "joint3", "joint4", "joint5", "joint6", "gripper"],
        # },
        # "action": {
        #     "dtype": "float32",
        #     "shape": (7,),
        #     "names": ["joint1", "joint2", "joint3", "joint4", "joint5", "joint6", "gripper"],
        # },
        #------------------ä¿®æ”¹--------------------#
        #------------------ä¿®æ”¹--------------------#
        #------------------ä¿®æ”¹--------------------#
        # Input State: ç°åœ¨ä»£è¡¨ã€ç›¸å¯¹äºæœ¬å›åˆèµ·å§‹ç‚¹çš„ä½ç§»ã€‘(Relative to Start)
        # å½¢çŠ¶: 7ç»´ (dx, dy, dz, dRx, dRy, dRz, gripper_abs)
        "observation.state": {
            "dtype": "float32",
            "shape": (7,), 
            "names": ["rel_x", "rel_y", "rel_z", "rel_roll", "rel_pitch", "rel_yaw", "gripper"],
        },
        
        # Output Action: ç°åœ¨ä»£è¡¨ã€ç›¸å¯¹äºä¸Šä¸€å¸§çš„å¢é‡ã€‘(Delta)
        # å½¢çŠ¶: 7ç»´ (dx, dy, dz, dRx, dRy, dRz, gripper_abs)
        "action": {
            "dtype": "float32",
            "shape": (7,),
            "names": ["dx", "dy", "dz", "dr", "dp", "dyaw", "gripper"],
        },
        
        # è¯­è¨€æŒ‡ä»¤
        "language_instruction": {
            "dtype": "string",
            "shape": (1,),
            "names": ["instruction"],
        },
    }
    
    # æ‘„åƒå¤´é…ç½®
    cameras = ["cam_left_wrist", "cam_right_wrist"]
    for cam in cameras:
        H, W = image_resolution
        features[f"observation.images.{cam}"] = {
            "dtype": "image",
            "shape": (3, H, W),
            "names": ["channels", "height", "width"],
        }
    
    # å¢é‡æ¨¡å¼ï¼šåŠ è½½å·²æœ‰æ•°æ®é›†ï¼›å…¨é‡æ¨¡å¼ï¼šåˆ›å»ºæ–°æ•°æ®é›†
    if incremental and output_path.exists():
        print(f"[å¢é‡æ¨¡å¼] åŠ è½½å·²æœ‰æ•°æ®é›†: {output_path}")
        dataset = LeRobotDataset(str(output_path))
        # é‡ç½®æ•°æ®é›†çŠ¶æ€ä»¥è¿½åŠ æ–°episode
        dataset._current_episode_frames = []
        dataset._current_task = None
    else:
        dataset = LeRobotDataset.create(
            repo_id=repo_id,
            root=output_path, 
            fps=10,
            robot_type=robot_type,
            features=features,
            use_videos=True, 
        )
    return dataset

def get_episode_number(episode_dir):
    """æå–episodeåç§°ä¸­çš„æ•°å­—ï¼ˆç”¨äºæ•°å€¼æ’åºï¼‰"""
    try:
        # ä» "episode_0" ä¸­æå– "0" å¹¶è½¬ä¸ºæ•´æ•°
        return int(episode_dir.name.split("_")[-1])
    except (ValueError, IndexError):
        # å¼‚å¸¸æƒ…å†µè¿”å›æå¤§å€¼ï¼Œæ’åˆ°æœ€å
        return float('inf')

def load_episode_data(episode_dir):
    """åŠ è½½å•ä¸ªepisodeçš„åŸå§‹æ•°æ®ï¼ˆä¿®å¤å›¾åƒç»Ÿè®¡é€»è¾‘ï¼‰"""
    print(f"\nğŸ“‚ å½“å‰å¤„ç†çš„episodeç»å¯¹è·¯å¾„: {episode_dir.absolute()}")
    data_file = episode_dir / "data.jsonl"
    
    if not data_file.exists():
        print(f"è­¦å‘Š: {episode_dir.name} ç¼ºå°‘data.jsonlæ–‡ä»¶ï¼Œè·³è¿‡")
        return None, None, None, None
    
    # å°è¯•åŠ è½½ jsonl
    try:
        with open(data_file, "r") as f:
            lines = [json.loads(line) for line in f]
    except Exception as e:
        print(f"âŒ è¯»å–jsonlæ–‡ä»¶å¤±è´¥: {e}")
        return None, None, None, None

    print(f"ğŸ“ {episode_dir.name} åŸå§‹data.jsonlè¡Œæ•°: {len(lines)}")
    
    cartesian_abs_list = [] 
    gripper_list = []
    instructions = []

    # é€è¡Œå¤„ç†å¹¶æ•è·é”™è¯¯
    for i, line in enumerate(lines):
        try:
            gripper_state = line.get("gripper_state", 0.0)
            
            # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
            if "cartesian_pos" not in line:
                print(f"âš ï¸ ç¬¬ {i} è¡Œç¼ºå°‘ 'cartesian_pos' å­—æ®µï¼Œè·³è¿‡ã€‚å†…å®¹: {line.keys()}")
                continue
                
            raw_cart = line["cartesian_pos"]
            
            # æ£€æŸ¥æ•°æ®é•¿åº¦
            if len(raw_cart) < 6:
                print(f"âš ï¸ ç¬¬ {i} è¡Œ 'cartesian_pos' é•¿åº¦ä¸è¶³: {raw_cart}")
                continue

            cart_pos = np.array(raw_cart, dtype=np.float32)
            cart_pos[:3] /= 1000.0  # mm -> m
            
            # åªæœ‰æ•°æ®æœ‰æ•ˆæ‰æ·»åŠ åˆ°åˆ—è¡¨
            cartesian_abs_list.append(cart_pos)
            gripper_list.append(gripper_state)
            instructions.append(line.get("instruction", ""))
            
        except Exception as e:
            print(f"âŒ å¤„ç†ç¬¬ {i} è¡Œæ•°æ®æ—¶å‡ºé”™: {e}")
            continue

    # æ£€æŸ¥æ˜¯å¦è§£æåˆ°äº†æ•°æ®
    if len(cartesian_abs_list) == 0:
        print(f"âŒ è­¦å‘Š: {episode_dir.name} è§£æåæœ‰æ•ˆæ•°æ®ä¸º0ï¼è¯·æ£€æŸ¥jsonlæ ¼å¼ã€‚")
        return None, None, None, None

    # è½¬ä¸º numpy æ•°ç»„
    cartesian_abs_arr = np.array(cartesian_abs_list, dtype=np.float32)
    gripper_arr = np.array(gripper_list, dtype=np.float32).reshape(-1, 1)
    
    print(f"âœ… æˆåŠŸè§£æçŠ¶æ€æ•°æ®: {cartesian_abs_arr.shape}")

    # ========== å›¾åƒåŠ è½½é€»è¾‘ ==========
    import re
    images = {}
    cameras = ["cam_left_wrist", "cam_right_wrist"]
    
    for cam in cameras:
        cam_dir = episode_dir / "images" / cam
        if not cam_dir.exists():
            print(f"è­¦å‘Š: {episode_dir.name} ç¼ºå°‘{cam}å›¾åƒç›®å½•")
            images[cam] = np.array([])
            continue
            
        import os
        os.sync()
        
        img_files = []
        for file in cam_dir.iterdir():
            if file.is_file() and file.suffix.lower() == ".jpg":
                img_files.append(file)
        
        def extract_number(file_path):
            nums = re.findall(r'\d+', file_path.name)
            return int(nums[0]) if nums else float('inf')
        
        img_files.sort(key=extract_number)
        
        # åŠ è½½å›¾åƒ
        cam_imgs = []
        for img_file in img_files:
            img = cv2.imread(str(img_file))
            if img is not None:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                cam_imgs.append(img)
        
        # é•¿åº¦å¯¹é½æ£€æŸ¥
        if len(cam_imgs) != len(cartesian_abs_arr):
            print(f"âš ï¸ {cam} å›¾åƒæ•°({len(cam_imgs)}) ä¸ çŠ¶æ€æ•°({len(cartesian_abs_arr)}) ä¸ä¸€è‡´")
            
        images[cam] = np.array(cam_imgs)
    
    return cartesian_abs_arr, gripper_arr, images, instructions

def convert_single_episode(episode_dir, dataset, sample_data_list):
    """è½¬æ¢å•ä¸ªepisodeå¹¶è¿½åŠ åˆ°æ•°æ®é›†"""
    # åŠ è½½æ•°æ®
    # st_abs, st_delta, images, instructions = load_episode_data(episode_dir)
    cart_abs, gripper, images, instructions = load_episode_data(episode_dir)
    if cart_abs is None:
        return False
    
    # è®¡ç®—æœ€å°é•¿åº¦ï¼ˆå¯¹é½æ‰€æœ‰æ•°æ®ï¼‰
    min_len = len(cart_abs)
    for cam, imgs in images.items():
        min_len = min(min_len, len(imgs)) if len(imgs) > 0 else min_len
    
    if min_len == 0:
        print(f"è­¦å‘Š: {episode_dir.name} æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
        return False
    
    # ç»Ÿä¸€è£åˆ‡åˆ°æœ€å°é•¿åº¦
    cart_abs = cart_abs[:min_len]
    gripper = gripper[:min_len]
    instructions = instructions[:min_len]
    for cam in images:
        if len(images[cam]) > 0:
            images[cam] = images[cam][:min_len]
    
    ## A. è®¡ç®— State: Relative to Start (å½“å‰ç»å¯¹ - ç¬¬ä¸€å¸§ç»å¯¹)
    start_pose = cart_abs[0] # è®°å½•èµ·å§‹ç‚¹
    # å¹¿æ’­å‡æ³•ï¼šæ¯ä¸€å¸§éƒ½å‡å»èµ·å§‹ç‚¹
    state_pos_rel = cart_abs - start_pose 
    # æ‹¼æ¥å¤¹çˆªï¼š[rel_x, rel_y, rel_z, rel_r, rel_p, rel_y, gripper]
    state_final_np = np.hstack([state_pos_rel, gripper])
    # B. è®¡ç®— Action: Delta (ä¸‹ä¸€å¸§ç»å¯¹ - å½“å‰å¸§ç»å¯¹)
    action_pos_delta = np.zeros_like(cart_abs)
    # Delta[t] = Pose[t+1] - Pose[t]
    action_pos_delta[:-1] = cart_abs[1:] - cart_abs[:-1]
    # æœ€åä¸€å¸§å¢é‡è®¾ä¸º0ï¼ˆæˆ–è€…å¤åˆ¶ä¸Šä¸€å¸§ï¼‰
    action_pos_delta[-1] = action_pos_delta[-2] if min_len > 1 else 0.0
    # Actionä¸­çš„å¤¹çˆªï¼šé¢„æµ‹ä¸‹ä¸€æ­¥çš„ç»å¯¹çŠ¶æ€
    action_gripper = np.zeros_like(gripper)
    action_gripper[:-1] = gripper[1:]
    action_gripper[-1] = gripper[-1]
    # æ‹¼æ¥ Action
    action_final_np = np.hstack([action_pos_delta, action_gripper])
    # æ‰“å°ä¿¡æ¯
    print(f"\n=== å¤„ç† Episode: {episode_dir.name} ===")
    print(f"æ¨¡å¼: Cartesian Relative State & Delta Action")
    print(f"Stateç»´åº¦: {state_final_np.shape}, Actionç»´åº¦: {action_final_np.shape}")
    # ä¿å­˜ç¤ºä¾‹æ•°æ® (ç”¨äºè°ƒè¯•æŸ¥çœ‹)
    if len(sample_data_list) < 5: # åªå­˜å‰å‡ ä¸ªé˜²æ­¢æ–‡ä»¶è¿‡å¤§
        sample_frame = {
            "episode": episode_dir.name,
            "frame_idx": 0,
            "state_sample": state_final_np[0].tolist(), # åº”è¯¥æ˜¯æ¥è¿‘0
            "action_sample": action_final_np[0].tolist(),
            "instruction": instructions[0]
        }
        sample_data_list.append(sample_frame)

    
    
    
    
    
    # å†™å…¥æ¯ä¸€å¸§æ•°æ®
    for i in range(min_len):
        frame = {
            # ç»å¯¹å€¼
            "observation.state": torch.from_numpy(state_final_np[i]),
            "action": torch.from_numpy(action_final_np[i]),
            # è¯­è¨€æŒ‡ä»¤
            "language_instruction": instructions[i],
        }
        
        # å¤„ç†æ¯ä¸ªæ‘„åƒå¤´çš„å›¾åƒï¼ˆå…¼å®¹ç¼ºå¤±æƒ…å†µï¼‰
        cameras = ["cam_left_wrist", "cam_right_wrist"]
        for cam in cameras:
            cam_imgs = images.get(cam, np.array([]))
            # å›¾åƒç¼ºå¤±/ç´¢å¼•è¶Šç•Œï¼šç”¨ç©ºå›¾åƒå¡«å……
            if len(cam_imgs) == 0 or i >= len(cam_imgs):
                img = create_empty_image(image_resolution)
            else:
                img = cam_imgs[i]
                # ç¼©æ”¾+è¡¥è¾¹åˆ°ç›®æ ‡åˆ†è¾¨ç‡
                if img.shape[:2] != image_resolution:
                    img = image_tools.resize_with_pad(img, *image_resolution)
                    img = np.array(img)
            frame[f"observation.images.{cam}"] = img
        
        dataset.add_frame(frame, task=instructions[i])
    
    # ä¿å­˜å½“å‰episodeï¼ˆå¢é‡å†™å…¥ï¼‰
    dataset.save_episode()
    print(f"âœ… {episode_dir.name} è½¬æ¢å®Œæˆå¹¶è¿½åŠ åˆ°æ•°æ®é›†")
    return True

def load_existing_sample_data(sample_file):
    """åŠ è½½å·²æœ‰çš„ç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºå¢é‡è¿½åŠ ï¼‰"""
    if not sample_file.exists():
        return []
    try:
        with open(sample_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        print(f"è­¦å‘Š: è¯»å–{sample_file}å¤±è´¥ï¼Œé‡æ–°åˆ›å»º")
        return []

def incremental_convert(raw_dir, repo_id, output_dir):
    """å¢é‡è½¬æ¢ä¸»å‡½æ•°ï¼šæŒç»­æ‰«ææ–°å¢episodeå¹¶è¿½åŠ """
    raw_dir = Path(raw_dir)
    output_dir = Path(output_dir)
    output_path = output_dir / repo_id
    sample_file = output_path / "sample_data.json"
    
    # åˆå§‹åŒ–ï¼šåˆ›å»º/åŠ è½½æ•°æ®é›†
    dataset = create_dataset(repo_id, output_dir, incremental=True)
    
    # åŠ è½½å·²è½¬æ¢çš„episodeåˆ—è¡¨
    converted_episodes = set()
    if output_path.exists():
        # ä»ç¤ºä¾‹æ•°æ®ä¸­è¯»å–å·²è½¬æ¢çš„episode
        sample_data = load_existing_sample_data(sample_file)
        converted_episodes = {item["episode"] for item in sample_data}
        print(f"ğŸ“Œ å·²è½¬æ¢çš„episodeæ•°é‡: {len(converted_episodes)}")
    
    # åŠ è½½ç¤ºä¾‹æ•°æ®åˆ—è¡¨
    sample_data_list = load_existing_sample_data(sample_file)
    
    print(f"\nğŸš€ å¯åŠ¨å¢é‡è½¬æ¢æ¨¡å¼ï¼Œæ¯éš”{SCAN_INTERVAL}ç§’æ‰«æä¸€æ¬¡æ–°å¢episode")
    print(f"åŸå§‹æ•°æ®ç›®å½•: {raw_dir}")
    print(f"è¾“å‡ºæ•°æ®é›†ç›®å½•: {output_path}")
    print("æŒ‰ Ctrl+C åœæ­¢è½¬æ¢\n")
    
    try:
         while True:
            # æ‰«ææ‰€æœ‰episodeç›®å½•
            all_episode_dirs = [
                d for d in raw_dir.iterdir() 
                if d.is_dir() and d.name.startswith("episode_")
            ]
            
            # ========== æ–°å¢ï¼šè¿‡æ»¤æ‰æœªå®Œæˆå†™å…¥çš„episode ==========
            completed_episodes = []
            for d in all_episode_dirs:
                # 1. æ£€æŸ¥data.jsonlæ˜¯å¦å­˜åœ¨ä¸”å†™å…¥å®Œæˆï¼ˆæœ€åä¿®æ”¹æ—¶é—´è¶…è¿‡2ç§’ï¼‰
                data_file = d / "data.jsonl"
                if not data_file.exists():
                    continue
                data_mtime = data_file.stat().st_mtime
                if time.time() - data_mtime < 2:  # å»¶è¿Ÿ2ç§’ï¼Œç¡®ä¿å†™å…¥å®Œæˆ
                    continue
                
                # 2. æ£€æŸ¥å›¾åƒç›®å½•æ˜¯å¦å­˜åœ¨ä¸”å†™å…¥å®Œæˆ
                # cam_dir = d / "images" / "cam_high"
                cam_dir = d / "images" / "cam_left_wrist"
                if not cam_dir.exists():
                    continue
                # å–æœ€åä¸€å¼ å›¾åƒçš„ä¿®æ”¹æ—¶é—´ï¼ˆéªŒè¯å†™å…¥å®Œæˆï¼‰
                img_files = list(cam_dir.glob("*.jpg"))
                if len(img_files) == 0:
                    continue
                last_img_mtime = max(f.stat().st_mtime for f in img_files)
                if time.time() - last_img_mtime < 2:
                    continue
                
                completed_episodes.append(d)
            
            # ========== æŒ‰æ•°å€¼æ’åº ==========
            completed_episodes.sort(key=get_episode_number)
            
            # ç­›é€‰æœªè½¬æ¢çš„episode
            new_episodes = [
                d for d in completed_episodes 
                if d.name not in converted_episodes
            ]
            
            # å¤„ç†æ–°å¢episode
            if new_episodes:
                # æ‰“å°æ–°å¢episodeåˆ—è¡¨ï¼ˆæŒ‰é¡ºåºï¼‰
                new_ep_names = [d.name for d in new_episodes]
                print(f"\nğŸ” å‘ç°{len(new_episodes)}ä¸ªæ–°å¢episode: {new_ep_names}")
                for ep_dir in tqdm.tqdm(new_episodes, desc="è½¬æ¢ä¸­"):
                    # è½¬æ¢å¹¶è¿½åŠ 
                    success = convert_single_episode(ep_dir, dataset, sample_data_list)
                    if success:
                        converted_episodes.add(ep_dir.name)
                        # å®æ—¶ä¿å­˜ç¤ºä¾‹æ•°æ®
                        with open(sample_file, "w", encoding="utf-8") as f:
                            json.dump(sample_data_list, f, indent=4, ensure_ascii=False)
            else:
                print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] æš‚æ— æ–°å¢episodeï¼Œç­‰å¾…ä¸­...", end="\r")
            
            # ç­‰å¾…ä¸‹ä¸€æ¬¡æ‰«æ
            time.sleep(SCAN_INTERVAL)
            
    except KeyboardInterrupt:
        print(f"\n\nğŸ›‘ ç”¨æˆ·ç»ˆæ­¢ç¨‹åº")
    finally:
        # æœ€ç»ˆä¿å­˜ç¤ºä¾‹æ•°æ®
        print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆç¤ºä¾‹æ•°æ®åˆ°: {sample_file}")
        with open(sample_file, "w", encoding="utf-8") as f:
            json.dump(sample_data_list, f, indent=4, ensure_ascii=False)
        # éªŒè¯æœ€ç»ˆæ•°æ®é›†
        print("\n=== éªŒè¯æœ€ç»ˆæ•°æ®é›† ===")
        final_dataset = LeRobotDataset(str(output_path))
        print(f"æ•°æ®é›†æ€»å¸§æ•°: {len(final_dataset)}")
        print(f"å·²è½¬æ¢episodeæ•°é‡: {len(converted_episodes)}")

def verify_converted_data(output_dir, repo_id):
    """éªŒè¯è½¬æ¢åçš„æ•°æ®é›†"""
    print("\n=== éªŒè¯æ•°æ®é›† ===")
    dataset_path = Path(output_dir) / repo_id
    if not dataset_path.exists():
        print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
        return
    
    dataset = LeRobotDataset(str(dataset_path))
    print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œæ€»å¸§æ•°: {len(dataset)}")
    
    if len(dataset) > 0:
        sample_data = dataset[0]
        print(f"\n--- ç¬¬0å¸§æ•°æ®æ ·ä¾‹ ---")
        print(f"observation.state: {np.round(sample_data['observation.state'], 4)}")
        print(f"action: {np.round(sample_data['action'], 4)}")
        print(f"language_instruction: {sample_data['language_instruction']}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--raw-dir", type=str, 
                        default="/home/openpi/data/data_raw/exp19_data_auto_queue_PutAndRecord_0113/raw", 
                        help="åŸå§‹æ•°æ®ç›®å½•")
    # parser.add_argument("--raw-dir", type=str, 
    #                     default="/home/openpi/data/data_raw/test/raw", 
    #                     help="åŸå§‹æ•°æ®ç›®å½•")
    parser.add_argument("--repo-id", type=str, 
                        default="xarm_autoPut_pi05_dataset", 
                        help="æ•°æ®é›†åç§°")
    parser.add_argument("--output-dir", type=str, 
                        default="/home/openpi/data/data_converted/exp19_lerobot_autoPut_data_0113night_224_224", 
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--scan-interval", type=int, default=5, 
                        help="æ‰«ææ–°å¢episodeçš„é—´éš”ï¼ˆç§’ï¼‰")
    args = parser.parse_args()
    
    # è¦†ç›–å…¨å±€æ‰«æé—´éš”
    SCAN_INTERVAL = args.scan_interval
    
    # å¯åŠ¨å¢é‡è½¬æ¢
    incremental_convert(args.raw_dir, args.repo_id, args.output_dir)
    
    # éªŒè¯æœ€ç»ˆç»“æœ
    verify_converted_data(args.output_dir, args.repo_id)
