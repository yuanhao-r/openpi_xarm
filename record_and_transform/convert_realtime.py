import argparse
import json
import shutil
import time
from pathlib import Path
import numpy as np
import cv2
import torch
import tqdm
from lerobot.datasets.lerobot_dataset import LeRobotDataset

import sys
# ç¡®ä¿åŒ…å« openpi æ–‡ä»¶å¤¹çš„ä¸Šä¸€å±‚ç›®å½• (src) åœ¨è·¯å¾„ä¸­
sys.path.append("/home/openpi/src")
from openpi.shared import image_tools

# é…ç½®é¡¹
SCALE_FACTOR = 1.0
image_resolution = (224, 224)
SCAN_INTERVAL = 5  # æ¯éš”5ç§’æ‰«æä¸€æ¬¡æ–°å¢episode
MAX_RETRY = 3      # å•ä¸ªepisodeè½¬æ¢å¤±è´¥é‡è¯•æ¬¡æ•°

# æ–°å¢ï¼šåˆ›å»ºç©ºå›¾åƒï¼ˆç”¨äºå¡«å……ç¼ºå¤±çš„æ‘„åƒå¤´æ•°æ®ï¼‰
def create_empty_image(resolution):
    """åˆ›å»ºé»‘è‰²ç©ºå›¾åƒï¼ˆRGBæ ¼å¼ï¼‰"""
    H, W = resolution
    return np.zeros((H, W, 3), dtype=np.uint8)

def create_dataset(repo_id, root_dir, robot_type="xarm", incremental=False):
    """åˆ›å»º/åŠ è½½æ•°æ®é›†ï¼ˆå¢é‡æ¨¡å¼ä¸åˆ é™¤æ—§æ•°æ®ï¼‰"""
    root_dir = Path(root_dir)
    output_path = root_dir / repo_id
    
    # å¢é‡æ¨¡å¼ï¼šä¿ç•™åŸæœ‰æ•°æ®ï¼Œä»…åŠ è½½ï¼›å…¨é‡æ¨¡å¼ï¼šæ¸…ç©ºé‡å»º
    if not incremental and output_path.exists():
        print(f"[å…¨é‡æ¨¡å¼] æ¸…ç†æ—§æ•°æ®é›†: {output_path}")
        shutil.rmtree(output_path)

    features = {
        # ç»å¯¹å€¼
        "observation.state": {
            "dtype": "float32",
            "shape": (7,), 
            "names": ["joint1", "joint2", "joint3", "joint4", "joint5", "joint6", "gripper"],
        },
        "action": {
            "dtype": "float32",
            "shape": (7,),
            "names": ["joint1", "joint2", "joint3", "joint4", "joint5", "joint6", "gripper"],
        },
        # ç›¸å¯¹å€¼
        "observation.state_delta": {
            "dtype": "float32",
            "shape": (7,), 
            "names": ["j1_d", "j2_d", "j3_d", "j4_d", "j5_d", "j6_d", "gripper"],
        },
        "action_delta": {
            "dtype": "float32",
            "shape": (7,),
            "names": ["j1_d", "j2_d", "j3_d", "j4_d", "j5_d", "j6_d", "gripper"],
        },
        # è¯­è¨€æŒ‡ä»¤
        "language_instruction": {
            "dtype": "string",
            "shape": (1,),
            "names": ["instruction"],
        },
    }
    
    # æ‘„åƒå¤´é…ç½®
    cameras = ["cam_high", "cam_left_wrist", "cam_right_wrist"]
    for cam in cameras:
        H, W = image_resolution
        features[f"observation.images.{cam}"] = {
            "dtype": "image",
            "shape": (3, H, W),
            "names": ["channels", "height", "width"],
        }
    
    # å¢é‡æ¨¡å¼ï¼šåŠ è½½å·²æœ‰æ•°æ®é›†ï¼›å…¨é‡æ¨¡å¼ï¼šåˆ›å»ºæ–°æ•°æ®é›†
    if incremental and output_path.exists():
        print(f"[å¢é‡æ¨¡å¼] åŠ è½½å·²æœ‰æ•°æ®é›†: {output_path}")
        dataset = LeRobotDataset(str(output_path))
        # é‡ç½®æ•°æ®é›†çŠ¶æ€ä»¥è¿½åŠ æ–°episode
        dataset._current_episode_frames = []
        dataset._current_task = None
    else:
        dataset = LeRobotDataset.create(
            repo_id=repo_id,
            root=output_path, 
            fps=10,
            robot_type=robot_type,
            features=features,
            use_videos=True, 
        )
    return dataset

def get_episode_number(episode_dir):
    """æå–episodeåç§°ä¸­çš„æ•°å­—ï¼ˆç”¨äºæ•°å€¼æ’åºï¼‰"""
    try:
        # ä» "episode_0" ä¸­æå– "0" å¹¶è½¬ä¸ºæ•´æ•°
        return int(episode_dir.name.split("_")[-1])
    except (ValueError, IndexError):
        # å¼‚å¸¸æƒ…å†µè¿”å›æå¤§å€¼ï¼Œæ’åˆ°æœ€å
        return float('inf')

def load_episode_data(episode_dir):
    """åŠ è½½å•ä¸ªepisodeçš„åŸå§‹æ•°æ®ï¼ˆä¿®å¤å›¾åƒç»Ÿè®¡é€»è¾‘ï¼‰"""
    # ========== 1. æ‰“å°å…³é”®è·¯å¾„ï¼ŒéªŒè¯è¯»å–çš„æ˜¯æ­£ç¡®æ–‡ä»¶ ==========
    print(f"\nğŸ“‚ å½“å‰å¤„ç†çš„episodeç»å¯¹è·¯å¾„: {episode_dir.absolute()}")
    data_file = episode_dir / "data.jsonl"
    print(f"ğŸ“„ data.jsonlç»å¯¹è·¯å¾„: {data_file.absolute()}")
    
    if not data_file.exists():
        print(f"è­¦å‘Š: {episode_dir.name} ç¼ºå°‘data.jsonlæ–‡ä»¶ï¼Œè·³è¿‡")
        return None, None, None, None
    
    # ========== 2. åŠ è½½å¹¶ç»Ÿè®¡data.jsonl ==========
    with open(data_file, "r") as f:
        lines = [json.loads(line) for line in f]
    print(f"ğŸ“ {episode_dir.name} åŸå§‹data.jsonlè¡Œæ•°: {len(lines)}")
    
    states_abs = []   # ç»å¯¹å€¼
    states_delta = [] # ç›¸å¯¹å€¼
    instructions = []
    
    for line in lines:
        gripper_state = line.get("gripper_state", 0.0)
        
        # 1. ç»å¯¹å€¼ï¼ˆå…³èŠ‚+å¤¹çˆªï¼‰
        if "joint_abs" in line:
            raw_abs = line["joint_abs"]
        else:
            raw_abs = line["joint_pos"]  # å…¼å®¹æ—§æ•°æ®
        abs_6d = np.array(raw_abs[:6], dtype=np.float32)
        final_abs = np.append(abs_6d, gripper_state)
        states_abs.append(final_abs)

        # 2. ç›¸å¯¹å€¼ï¼ˆå…³èŠ‚+å¤¹çˆªï¼‰
        if "joint_abs" in line:
            raw_delta = line["joint_pos"]
        else:
            raw_delta = [0.0] * 7 
        delta_6d = np.array(raw_delta[:6], dtype=np.float32) * SCALE_FACTOR
        final_delta = np.append(delta_6d, gripper_state)
        states_delta.append(final_delta)

        instructions.append(line.get("instruction", ""))
    
    # ç©ºæ•°æ®æ£€æŸ¥
    if len(states_abs) == 0:
        print(f"è­¦å‘Š: {episode_dir.name} æ— æœ‰æ•ˆçŠ¶æ€æ•°æ®ï¼Œè·³è¿‡")
        return None, None, None, None
        
    # è½¬ä¸ºnumpyæ•°ç»„
    states_abs = np.array(states_abs, dtype=np.float32)
    states_delta = np.array(states_delta, dtype=np.float32)
    print(f"ğŸ“Š {episode_dir.name} æœ‰æ•ˆçŠ¶æ€æ•°: {len(states_abs)}")
    
    # ========== 3. ä¿®å¤å›¾åƒåŠ è½½é€»è¾‘ï¼šç²¾å‡†ç»Ÿè®¡æ‰€æœ‰.jpgæ–‡ä»¶ ==========
    import re  # ç”¨äºæå–æ–‡ä»¶åä¸­çš„æ•°å­—
    images = {}
    cameras = ["cam_high", "cam_left_wrist", "cam_right_wrist"]
    
    # ... åŠ è½½data.jsonlå ...
    # æå–æ‰€æœ‰frame_idxï¼Œç¡®å®šç›®æ ‡æ•°é‡
    frame_ids = [line["frame_idx"] for line in lines]
    max_frame_id = max(frame_ids) if frame_ids else 0
    target_count = max_frame_id + 1  # å¦‚frame_idx 0-25 â†’ 26ä¸ª
    
    for cam in cameras:
        cam_dir = episode_dir / "images" / cam
        print(f"\nğŸ“· {cam} å›¾åƒç›®å½•ç»å¯¹è·¯å¾„: {cam_dir.absolute()}")
        
        if not cam_dir.exists():
            print(f"è­¦å‘Š: {episode_dir.name} ç¼ºå°‘{cam}å›¾åƒç›®å½•")
            images[cam] = np.array([])
            continue
        # ========== æ–°å¢ï¼šå¼ºåˆ¶åˆ·æ–°æ–‡ä»¶ç³»ç»Ÿç¼“å­˜ ==========
        import os
        os.sync()  # åˆ·æ–°ç³»ç»Ÿç¼“å­˜ï¼Œç¡®ä¿æ‰€æœ‰å†™å…¥çš„æ–‡ä»¶è¢«è¯†åˆ«
        time.sleep(0.1)  # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…ç¼“å­˜æœªåŒæ­¥
        
        # éå†æ‰€æœ‰.jpg/.JPGæ–‡ä»¶ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        img_files = []
        for file in cam_dir.iterdir():
            if file.is_file() and file.suffix.lower() == ".jpg":
                img_files.append(file)
        
        # æŒ‰æ–‡ä»¶åä¸­çš„æ•°å­—æ’åºï¼ˆå…¼å®¹ä»»æ„å‘½åæ ¼å¼ï¼‰
        def extract_number(file_path):
            """ä»æ–‡ä»¶åæå–æ•°å­—ï¼ˆå¦‚img_001.jpg â†’ 1ï¼Œ2.jpg â†’ 2ï¼‰"""
            nums = re.findall(r'\d+', file_path.name)
            return int(nums[0]) if nums else float('inf')
        
        img_files.sort(key=extract_number)
        
        # æ‰“å°æ‰€æœ‰å›¾åƒæ–‡ä»¶åï¼ˆå…³é”®ï¼šå®šä½ç¼ºå¤±çš„æ–‡ä»¶ï¼‰
        img_names = [f.name for f in img_files]
        print(f"ğŸ“¸ {episode_dir.name} {cam} åŸå§‹å›¾åƒæ•°: {len(img_files)}")
        print(f"   æ‰€æœ‰å›¾åƒæ–‡ä»¶å: {img_names}")
        
        # åŠ è½½å›¾åƒï¼ˆè¿‡æ»¤æŸåæ–‡ä»¶ï¼‰
        cam_imgs = []
        invalid_imgs = []
        for img_file in img_files:
            img = cv2.imread(str(img_file))
            if img is None:
                invalid_imgs.append(img_file.name)
                continue
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            cam_imgs.append(img)
        
        # æ‰“å°æ— æ•ˆå›¾åƒï¼ˆå¦‚æœæœ‰ï¼‰
        if invalid_imgs:
            print(f"âŒ {episode_dir.name} {cam} æ— æ•ˆå›¾åƒ: {invalid_imgs}")
        
        print(f"âœ… {episode_dir.name} {cam} æœ‰æ•ˆå›¾åƒæ•°: {len(cam_imgs)}")
        
        # å›¾åƒ-çŠ¶æ€é•¿åº¦å¯¹é½
        if len(cam_imgs) != len(states_abs):
            diff = abs(len(cam_imgs) - len(states_abs))
            print(f"è­¦å‘Š: {episode_dir.name} {cam}å›¾åƒæ•°({len(cam_imgs)})ä¸çŠ¶æ€æ•°({len(states_abs)})ä¸åŒ¹é…ï¼ˆå·®å€¼{diff}ï¼‰")
            min_len = min(len(cam_imgs), len(states_abs))
            cam_imgs = cam_imgs[:min_len]
            
        images[cam] = np.array(cam_imgs)
    
    return states_abs, states_delta, images, instructions


def convert_single_episode(episode_dir, dataset, sample_data_list):
    """è½¬æ¢å•ä¸ªepisodeå¹¶è¿½åŠ åˆ°æ•°æ®é›†"""
    # åŠ è½½æ•°æ®
    st_abs, st_delta, images, instructions = load_episode_data(episode_dir)
    if st_abs is None:
        return False
    
    # è®¡ç®—æœ€å°é•¿åº¦ï¼ˆå¯¹é½æ‰€æœ‰æ•°æ®ï¼‰
    min_len = len(st_abs)
    for cam, imgs in images.items():
        min_len = min(min_len, len(imgs)) if len(imgs) > 0 else min_len
    
    if min_len == 0:
        print(f"è­¦å‘Š: {episode_dir.name} æ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
        return False
    
    # ç»Ÿä¸€è£åˆ‡åˆ°æœ€å°é•¿åº¦
    st_abs = st_abs[:min_len]
    st_delta = st_delta[:min_len]
    instructions = instructions[:min_len]
    for cam in images:
        if len(images[cam]) > 0:
            images[cam] = images[cam][:min_len]
    
    # æ„å»ºActionï¼ˆç»å¯¹å€¼/ç›¸å¯¹å€¼ï¼‰
    act_abs = np.zeros_like(st_abs)
    act_abs[:-1] = st_abs[1:]  # actionæ˜¯ä¸‹ä¸€å¸§çš„state
    act_abs[-1] = st_abs[-1]   # æœ€åä¸€å¸§é‡å¤
    
    act_delta = np.zeros_like(st_delta)
    act_delta[:-1] = st_delta[1:]
    act_delta[-1] = st_delta[-1]
    
    # æ‰“å°å½“å‰episodeä¿¡æ¯
    print(f"\n=== å¤„ç† Episode: {episode_dir.name} ===")
    print(f"æœ‰æ•ˆå¸§æ•°: {min_len}")
    print(f"observation.state ç»´åº¦: {st_abs.shape}")
    print(f"action ç»´åº¦: {act_abs.shape}")
    
    # ä¿å­˜ç¤ºä¾‹æ•°æ®ï¼ˆç¬¬0å¸§ï¼‰
    sample_frame = {
        "episode": episode_dir.name,
        "frame_idx": 0,
        "observation.state": st_abs[0].tolist(),
        "observation.state_delta": st_delta[0].tolist(),
        "action": act_abs[0].tolist(),
        "action_delta": act_delta[0].tolist(),
        "instruction": instructions[0]
    }
    sample_data_list.append(sample_frame)
    
    # å†™å…¥æ¯ä¸€å¸§æ•°æ®
    for i in range(min_len):
        frame = {
            # ç»å¯¹å€¼
            "observation.state": torch.from_numpy(st_abs[i]),
            "action": torch.from_numpy(act_abs[i]),
            # ç›¸å¯¹å€¼
            "observation.state_delta": torch.from_numpy(st_delta[i]),
            "action_delta": torch.from_numpy(act_delta[i]),
            # è¯­è¨€æŒ‡ä»¤
            "language_instruction": instructions[i],
        }
        
        # å¤„ç†æ¯ä¸ªæ‘„åƒå¤´çš„å›¾åƒï¼ˆå…¼å®¹ç¼ºå¤±æƒ…å†µï¼‰
        cameras = ["cam_high", "cam_left_wrist", "cam_right_wrist"]
        for cam in cameras:
            cam_imgs = images.get(cam, np.array([]))
            # å›¾åƒç¼ºå¤±/ç´¢å¼•è¶Šç•Œï¼šç”¨ç©ºå›¾åƒå¡«å……
            if len(cam_imgs) == 0 or i >= len(cam_imgs):
                img = create_empty_image(image_resolution)
            else:
                img = cam_imgs[i]
                # ç¼©æ”¾+è¡¥è¾¹åˆ°ç›®æ ‡åˆ†è¾¨ç‡
                if img.shape[:2] != image_resolution:
                    img = image_tools.resize_with_pad(img, *image_resolution)
                    img = np.array(img)
            frame[f"observation.images.{cam}"] = img
        
        dataset.add_frame(frame, task=instructions[i])
    
    # ä¿å­˜å½“å‰episodeï¼ˆå¢é‡å†™å…¥ï¼‰
    dataset.save_episode()
    print(f"âœ… {episode_dir.name} è½¬æ¢å®Œæˆå¹¶è¿½åŠ åˆ°æ•°æ®é›†")
    return True

def load_existing_sample_data(sample_file):
    """åŠ è½½å·²æœ‰çš„ç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºå¢é‡è¿½åŠ ï¼‰"""
    if not sample_file.exists():
        return []
    try:
        with open(sample_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        print(f"è­¦å‘Š: è¯»å–{sample_file}å¤±è´¥ï¼Œé‡æ–°åˆ›å»º")
        return []

def incremental_convert(raw_dir, repo_id, output_dir):
    """å¢é‡è½¬æ¢ä¸»å‡½æ•°ï¼šæŒç»­æ‰«ææ–°å¢episodeå¹¶è¿½åŠ """
    raw_dir = Path(raw_dir)
    output_dir = Path(output_dir)
    output_path = output_dir / repo_id
    sample_file = output_path / "sample_data.json"
    
    # åˆå§‹åŒ–ï¼šåˆ›å»º/åŠ è½½æ•°æ®é›†
    dataset = create_dataset(repo_id, output_dir, incremental=True)
    
    # åŠ è½½å·²è½¬æ¢çš„episodeåˆ—è¡¨
    converted_episodes = set()
    if output_path.exists():
        # ä»ç¤ºä¾‹æ•°æ®ä¸­è¯»å–å·²è½¬æ¢çš„episode
        sample_data = load_existing_sample_data(sample_file)
        converted_episodes = {item["episode"] for item in sample_data}
        print(f"ğŸ“Œ å·²è½¬æ¢çš„episodeæ•°é‡: {len(converted_episodes)}")
    
    # åŠ è½½ç¤ºä¾‹æ•°æ®åˆ—è¡¨
    sample_data_list = load_existing_sample_data(sample_file)
    
    print(f"\nğŸš€ å¯åŠ¨å¢é‡è½¬æ¢æ¨¡å¼ï¼Œæ¯éš”{SCAN_INTERVAL}ç§’æ‰«æä¸€æ¬¡æ–°å¢episode")
    print(f"åŸå§‹æ•°æ®ç›®å½•: {raw_dir}")
    print(f"è¾“å‡ºæ•°æ®é›†ç›®å½•: {output_path}")
    print("æŒ‰ Ctrl+C åœæ­¢è½¬æ¢\n")
    
    try:
         while True:
            # æ‰«ææ‰€æœ‰episodeç›®å½•
            all_episode_dirs = [
                d for d in raw_dir.iterdir() 
                if d.is_dir() and d.name.startswith("episode_")
            ]
            
            # ========== æ–°å¢ï¼šè¿‡æ»¤æ‰æœªå®Œæˆå†™å…¥çš„episode ==========
            completed_episodes = []
            for d in all_episode_dirs:
                # 1. æ£€æŸ¥data.jsonlæ˜¯å¦å­˜åœ¨ä¸”å†™å…¥å®Œæˆï¼ˆæœ€åä¿®æ”¹æ—¶é—´è¶…è¿‡2ç§’ï¼‰
                data_file = d / "data.jsonl"
                if not data_file.exists():
                    continue
                data_mtime = data_file.stat().st_mtime
                if time.time() - data_mtime < 2:  # å»¶è¿Ÿ2ç§’ï¼Œç¡®ä¿å†™å…¥å®Œæˆ
                    continue
                
                # 2. æ£€æŸ¥å›¾åƒç›®å½•æ˜¯å¦å­˜åœ¨ä¸”å†™å…¥å®Œæˆ
                cam_dir = d / "images" / "cam_high"
                if not cam_dir.exists():
                    continue
                # å–æœ€åä¸€å¼ å›¾åƒçš„ä¿®æ”¹æ—¶é—´ï¼ˆéªŒè¯å†™å…¥å®Œæˆï¼‰
                img_files = list(cam_dir.glob("*.jpg"))
                if len(img_files) == 0:
                    continue
                last_img_mtime = max(f.stat().st_mtime for f in img_files)
                if time.time() - last_img_mtime < 2:
                    continue
                
                completed_episodes.append(d)
            
            # ========== æŒ‰æ•°å€¼æ’åº ==========
            completed_episodes.sort(key=get_episode_number)
            
            # ç­›é€‰æœªè½¬æ¢çš„episode
            new_episodes = [
                d for d in completed_episodes 
                if d.name not in converted_episodes
            ]
            
            # å¤„ç†æ–°å¢episode
            if new_episodes:
                # æ‰“å°æ–°å¢episodeåˆ—è¡¨ï¼ˆæŒ‰é¡ºåºï¼‰
                new_ep_names = [d.name for d in new_episodes]
                print(f"\nğŸ” å‘ç°{len(new_episodes)}ä¸ªæ–°å¢episode: {new_ep_names}")
                for ep_dir in tqdm.tqdm(new_episodes, desc="è½¬æ¢ä¸­"):
                    # è½¬æ¢å¹¶è¿½åŠ 
                    success = convert_single_episode(ep_dir, dataset, sample_data_list)
                    if success:
                        converted_episodes.add(ep_dir.name)
                        # å®æ—¶ä¿å­˜ç¤ºä¾‹æ•°æ®
                        with open(sample_file, "w", encoding="utf-8") as f:
                            json.dump(sample_data_list, f, indent=4, ensure_ascii=False)
            else:
                print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] æš‚æ— æ–°å¢episodeï¼Œç­‰å¾…ä¸­...", end="\r")
            
            # ç­‰å¾…ä¸‹ä¸€æ¬¡æ‰«æ
            time.sleep(SCAN_INTERVAL)
            
    except KeyboardInterrupt:
        print(f"\n\nğŸ›‘ ç”¨æˆ·ç»ˆæ­¢ç¨‹åº")
    finally:
        # æœ€ç»ˆä¿å­˜ç¤ºä¾‹æ•°æ®
        print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆç¤ºä¾‹æ•°æ®åˆ°: {sample_file}")
        with open(sample_file, "w", encoding="utf-8") as f:
            json.dump(sample_data_list, f, indent=4, ensure_ascii=False)
        # éªŒè¯æœ€ç»ˆæ•°æ®é›†
        print("\n=== éªŒè¯æœ€ç»ˆæ•°æ®é›† ===")
        final_dataset = LeRobotDataset(str(output_path))
        print(f"æ•°æ®é›†æ€»å¸§æ•°: {len(final_dataset)}")
        print(f"å·²è½¬æ¢episodeæ•°é‡: {len(converted_episodes)}")

def verify_converted_data(output_dir, repo_id):
    """éªŒè¯è½¬æ¢åçš„æ•°æ®é›†"""
    print("\n=== éªŒè¯æ•°æ®é›† ===")
    dataset_path = Path(output_dir) / repo_id
    if not dataset_path.exists():
        print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
        return
    
    dataset = LeRobotDataset(str(dataset_path))
    print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œæ€»å¸§æ•°: {len(dataset)}")
    
    if len(dataset) > 0:
        sample_data = dataset[0]
        print(f"\n--- ç¬¬0å¸§æ•°æ®æ ·ä¾‹ ---")
        print(f"observation.state: {np.round(sample_data['observation.state'], 4)}")
        print(f"action: {np.round(sample_data['action'], 4)}")
        print(f"language_instruction: {sample_data['language_instruction']}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--raw-dir", type=str, 
                        default="/home/openpi/data/data_raw/exp8_data_auto_queue_PutAndRecord_1224/raw", 
                        help="åŸå§‹æ•°æ®ç›®å½•")
    parser.add_argument("--repo-id", type=str, 
                        default="xarm_autoPut_pi05_dataset", 
                        help="æ•°æ®é›†åç§°")
    parser.add_argument("--output-dir", type=str, 
                        default="/home/openpi/data/data_converted/exp8_lerobot_autoPut_data_1223morning_224_224", 
                        help="è¾“å‡ºç›®å½•")
    parser.add_argument("--scan-interval", type=int, default=5, 
                        help="æ‰«ææ–°å¢episodeçš„é—´éš”ï¼ˆç§’ï¼‰")
    args = parser.parse_args()
    
    # è¦†ç›–å…¨å±€æ‰«æé—´éš”
    SCAN_INTERVAL = args.scan_interval
    
    # å¯åŠ¨å¢é‡è½¬æ¢
    incremental_convert(args.raw_dir, args.repo_id, args.output_dir)
    
    # éªŒè¯æœ€ç»ˆç»“æœ
    verify_converted_data(args.output_dir, args.repo_id)
